{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b987f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import esm\n",
    "from esm.data import *\n",
    "from esm.model.esm2_secondarystructure import ESM2 as ESM2_SISS\n",
    "from esm.model.esm2_supervised import ESM2\n",
    "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer\n",
    "from esm.modules import ConvTransformerLayer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import r2_score, f1_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from collections import Counter\n",
    "# os.chdir('/root/devdata/pansc/github/UTR_Insight')\n",
    "\n",
    "global layers, heads, embed_dim, batch_toks, cnn_layers, alphabet\n",
    "\n",
    "from esm.modules import * \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be79d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 6\n",
    "heads = 16\n",
    "embed_dim =64\n",
    "# batch_toks = 4096*2 #4096\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "repr_layers = [0, layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585ce3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicFusionGate(nn.Module):\n",
    "    \"\"\"Dynamic fusion gate with multi-head attention style computation.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads   \n",
    "        # Query, Key projections\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(2 * embed_dim, embed_dim)\n",
    "        # Value projections for both branches\n",
    "        self.value_attn = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_conv = nn.Linear(embed_dim, embed_dim)\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        # Scale factor\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x_attn, x_conv):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_attn: Transformer branch output [B, T, C]\n",
    "            x_conv: CNN branch output [B, T, C]\n",
    "        Returns:\n",
    "            fused_output: [B, T, C]\n",
    "            gate_values: [B, T, 1] fusion weights for analysis\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x_attn.shape\n",
    "        \n",
    "        # Concatenate features for key\n",
    "        x_cat = torch.cat([x_attn, x_conv], dim=-1)  # [B, T, 2C]\n",
    "        # Project queries and keys\n",
    "        q = self.query(x_attn).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [B, H, T, D]\n",
    "        k = self.key(x_cat).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [B, H, T, D]\n",
    "        # Compute attention scores (gate weights)\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale  # [B, H, T, T]\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # [B, H, T, T]\n",
    "        # Project values for both branches\n",
    "        v_attn = self.value_attn(x_attn).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [B, H, T, D]\n",
    "        v_conv = self.value_conv(x_conv).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [B, H, T, D]\n",
    "        # Apply attention to values\n",
    "        attended_attn = (attn_weights @ v_attn).transpose(1, 2).reshape(batch_size, seq_len, self.embed_dim)\n",
    "        attended_conv = ((1 - attn_weights) @ v_conv).transpose(1, 2).reshape(batch_size, seq_len, self.embed_dim)\n",
    "        # Combine attended features\n",
    "        fused_output = attended_attn + attended_conv\n",
    "        fused_output = self.out_proj(fused_output)\n",
    "        # Compute average gate values for analysis\n",
    "        gate_values = attn_weights.mean(dim=1).mean(dim=-1, keepdim=True)  # [B, T, 1]\n",
    "        return fused_output, gate_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de20cdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100, 128])\n"
     ]
    }
   ],
   "source": [
    "class KmerSpecificExpert(nn.Module):\n",
    "    \"\"\"Expert specialized for processing features at a specific k-mer scale.\"\"\"\n",
    "    def __init__(self, embed_dim, ffn_embed_dim, kmer_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.kmer_size = kmer_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_embed_dim = ffn_embed_dim\n",
    "        # K-mer specific processing\n",
    "        self.conv_proj = nn.Conv1d(\n",
    "            embed_dim, embed_dim, \n",
    "            kmer_size, \n",
    "            padding=(kmer_size - 1) // 2\n",
    "        )\n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ffn_embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [num_selected_tokens, embed_dim]\n",
    "        x = x.unsqueeze(1)  # [num_selected_tokens, 1, embed_dim]\n",
    "        # Process with k-mer specific convolution\n",
    "        residual = x\n",
    "        x = x.permute(0, 2, 1)  # [num_selected_tokens, embed_dim, 1]\n",
    "        x = self.conv_proj(x)\n",
    "        x = x.permute(0, 2, 1)  # [num_selected_tokens, 1, embed_dim]\n",
    "        x = self.layer_norm(x + residual)\n",
    "        x = x.squeeze(1)  # [num_selected_tokens, embed_dim]\n",
    "        # Process with FFN\n",
    "        residual = x\n",
    "        x = self.ffn(x)\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class MOELayerWithKmerExperts(nn.Module):\n",
    "    \"\"\"Mixture of Experts layer with k-mer specific experts.\"\"\"\n",
    "    def __init__(self, embed_dim, ffn_embed_dim, num_experts=4, top_k=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.expert_kmer_sizes = [3, 5, 7, 9]  # Different k-mer sizes for each expert\n",
    "        # Create experts with different k-mer specializations\n",
    "        self.experts = nn.ModuleList([\n",
    "            KmerSpecificExpert(embed_dim, ffn_embed_dim, kmer, dropout)\n",
    "            for kmer in self.expert_kmer_sizes[:num_experts]\n",
    "        ])\n",
    "        # Gate to select experts\n",
    "        self.gate = nn.Linear(embed_dim, num_experts)\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        # Compute gate scores\n",
    "        gate_scores = self.gate(x)  # [B, T, num_experts]\n",
    "        gate_weights = F.softmax(gate_scores, dim=-1)  # [B, T, num_experts]\n",
    "        # Get top-k experts\n",
    "        top_k_weights, top_k_indices = torch.topk(\n",
    "            gate_weights, self.top_k, dim=-1, sorted=False\n",
    "        )  # [B, T, top_k]\n",
    "        # Normalize top-k weights\n",
    "        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)\n",
    "        # Flatten the batch and sequence dimensions for expert processing\n",
    "        flat_x = x.view(-1, self.embed_dim)  # [B*T, C]\n",
    "        flat_top_k_indices = top_k_indices.view(-1, self.top_k)  # [B*T, top_k]\n",
    "        flat_top_k_weights = top_k_weights.view(-1, self.top_k)  # [B*T, top_k]\n",
    "        \n",
    "        # Initialize output\n",
    "        output = torch.zeros_like(flat_x)\n",
    "        # Process with each expert\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create mask for current expert\n",
    "            expert_mask = (flat_top_k_indices == i).any(dim=-1)  # [B*T]\n",
    "            if expert_mask.any():\n",
    "                # Process relevant tokens with current expert\n",
    "                expert_input = flat_x[expert_mask]  # [M, C]\n",
    "                expert_output = expert(expert_input)\n",
    "                # Get weights for this expert\n",
    "                expert_weight = flat_top_k_weights[expert_mask] * \\\n",
    "                               (flat_top_k_indices[expert_mask] == i).float()  # [M, top_k]\n",
    "                expert_weight = expert_weight.sum(dim=-1, keepdim=True)  # [M, 1]\n",
    "                # Add weighted expert output\n",
    "                output[expert_mask] += expert_output * expert_weight\n",
    "        # Reshape output back to original dimensions\n",
    "        output = output.view(batch_size, seq_len, self.embed_dim)\n",
    "        # Project and return output\n",
    "        output = self.out_proj(output)\n",
    "        return self.dropout(output)\n",
    "\n",
    "class ParallelConvTransformerLayer(nn.Module):\n",
    "    \"\"\"Parallel CNN and Transformer layer with k-mer specific MOE and dynamic fusion.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        ffn_embed_dim,\n",
    "        attention_heads,\n",
    "        kmer=7,\n",
    "        dropout=0.1,\n",
    "        add_bias_kv=True,\n",
    "        use_esm1b_layer_norm=True,\n",
    "        use_rotary_embeddings=False,\n",
    "        num_experts=4,  # MOE parameters\n",
    "        top_k=2,       # MOE parameters\n",
    "        use_moe=False,  # Whether to use MOE\n",
    "        use_dynamic_fusion=True,  # Whether to use dynamic fusion\n",
    "        fusion_heads=4  # Number of heads for dynamic fusion\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_embed_dim = ffn_embed_dim\n",
    "        self.attention_heads = attention_heads\n",
    "        self.kmer = kmer\n",
    "        self.dropout = dropout\n",
    "        self.use_rotary_embeddings = use_rotary_embeddings\n",
    "        self.use_moe = use_moe\n",
    "        self.use_dynamic_fusion = use_dynamic_fusion\n",
    "        self.fusion_heads = fusion_heads\n",
    "        \n",
    "        BertLayerNorm = ESM1bLayerNorm if use_esm1b_layer_norm else ESM1LayerNorm\n",
    "\n",
    "        # Transformer branch\n",
    "        self.self_attn = MultiheadAttention(\n",
    "            self.embed_dim,\n",
    "            self.attention_heads,\n",
    "            add_bias_kv=add_bias_kv,\n",
    "            add_zero_attn=False,\n",
    "        )\n",
    "        self.attn_layer_norm = BertLayerNorm(self.embed_dim)\n",
    "        \n",
    "        # CNN branch\n",
    "        self.conv = nn.Conv1d(\n",
    "            self.embed_dim, \n",
    "            self.embed_dim, \n",
    "            self.kmer, \n",
    "            padding=(self.kmer - 1) // 2\n",
    "        )\n",
    "        self.conv_layer_norm = BertLayerNorm(self.embed_dim)\n",
    "        \n",
    "        # Fusion mechanism\n",
    "        if use_dynamic_fusion:\n",
    "            self.fusion = DynamicFusionGate(embed_dim, num_heads=fusion_heads)\n",
    "        else:\n",
    "            self.fusion_gate = nn.Sequential(\n",
    "                nn.Linear(2 * embed_dim, embed_dim),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        # FFN - either standard or MOE with k-mer experts\n",
    "        if use_moe:\n",
    "            self.ffn = MOELayerWithKmerExperts(\n",
    "                embed_dim, \n",
    "                ffn_embed_dim, \n",
    "                num_experts=num_experts, \n",
    "                top_k=top_k, \n",
    "                dropout=dropout\n",
    "            )\n",
    "        else:\n",
    "            self.ffn = nn.Sequential(\n",
    "                nn.Linear(self.embed_dim, self.ffn_embed_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(self.ffn_embed_dim, self.embed_dim),\n",
    "                nn.Dropout(self.dropout)\n",
    "            )\n",
    "            \n",
    "        self.final_layer_norm = BertLayerNorm(self.embed_dim)\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, x, self_attn_mask=None, self_attn_padding_mask=None, need_head_weights=False\n",
    "    ):\n",
    "        residual = x\n",
    "        \n",
    "        # Create inverse padding mask correctly\n",
    "        if self_attn_padding_mask is not None:\n",
    "            padding_mask = self_attn_padding_mask.bool() if not self_attn_padding_mask.is_floating_point() else self_attn_padding_mask > 0.5\n",
    "            inverse_padding_mask = ~padding_mask\n",
    "            inverse_padding_mask = inverse_padding_mask.unsqueeze(2).float()  # B*T*1\n",
    "        else:\n",
    "            inverse_padding_mask = torch.ones(x.shape[0], x.shape[1], 1, device=x.device)\n",
    "        \n",
    "        # Parallel branches\n",
    "        ## Transformer branch\n",
    "        x_attn = self.attn_layer_norm(x)\n",
    "        x_attn = x_attn.permute(1, 0, 2)  # T*B*C\n",
    "        x_attn, attn = self.self_attn(\n",
    "            query=x_attn,\n",
    "            key=x_attn,\n",
    "            value=x_attn,\n",
    "            key_padding_mask=self_attn_padding_mask,\n",
    "            need_weights=True,\n",
    "            attn_mask=self_attn_mask,\n",
    "        )\n",
    "        x_attn = x_attn.permute(1, 0, 2)  # B*T*C\n",
    "        \n",
    "        ## CNN branch\n",
    "        x_conv = x.permute(0, 2, 1)  # B*C*T\n",
    "        x_conv = self.conv(x_conv)  # B*C*T\n",
    "        x_conv = x_conv.permute(0, 2, 1)  # B*T*C\n",
    "        x_conv = self.conv_layer_norm(x_conv)\n",
    "        x_conv = x_conv * inverse_padding_mask\n",
    "        \n",
    "        # Fusion\n",
    "        if self.use_dynamic_fusion:\n",
    "            x, gate_values = self.fusion(x_attn, x_conv)\n",
    "        else:\n",
    "            fusion_input = torch.cat([x_attn, x_conv], dim=-1)  # B*T*2C\n",
    "            gate = self.fusion_gate(fusion_input)  # B*T*C\n",
    "            x = gate * x_attn + (1 - gate) * x_conv  # Gated fusion\n",
    "        \n",
    "        # Residual\n",
    "        x = residual + x\n",
    "        residual = x\n",
    "        \n",
    "        # FFN (either standard or MOE with k-mer experts)\n",
    "        x = self.final_layer_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        return x, attn\n",
    "\n",
    "\n",
    "# Test the ParallelConvTransformerLayer\n",
    "x = torch.rand(5, 100, 128).to(device)\n",
    "pad = torch.zeros_like(x[...,0]).bool().to(device)\n",
    "module = ParallelConvTransformerLayer(128, embed_dim*4, 8, kmer=7, dropout=0.1, use_moe=True, num_experts=4, top_k=2).to(device)\n",
    "x, attn = module(x, self_attn_padding_mask=pad)\n",
    "print(x.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc9275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8634],\n",
       "        [-0.0563],\n",
       "        [ 2.9372],\n",
       "        [ 0.6452],\n",
       "        [ 1.1146]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MainModel(nn.Module):\n",
    "    def __init__(self, alphabet, dropout=0.2, CovTransformer_layers=3, \n",
    "                 kmer=7, layers=6, embed_dim=128, nodes=40, heads=16):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embed_dim\n",
    "        self.nodes = nodes\n",
    "        self.dropout = dropout\n",
    "        self.esm2 = ESM2_SISS(num_layers = layers,\n",
    "                        embed_dim = embed_dim,\n",
    "                        attention_heads = heads,\n",
    "                        alphabet = alphabet) \n",
    "        # Change to nn.ModuleList\n",
    "        self.convtransformer_Feature Fusion Module = nn.ModuleList([\n",
    "            ParallelConvTransformerLayer(embed_dim, embed_dim*4, heads, kmer, dropout=self.dropout, \n",
    "                                         use_moe=True, num_experts=4, top_k=2\n",
    "                                         )\n",
    "            for i in range(CovTransformer_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Linear layer for processing experimental source information\n",
    "        self.experiment_dense = nn.Linear(2, self.nodes)  # Handling one-hot experiment indicators\n",
    "        self.linear = nn.Linear(in_features = 6 * embed_dim, out_features = self.nodes)\n",
    "        self.linear_2 = nn.Linear(in_features = self.nodes, out_features = self.nodes * 4)\n",
    "        self.linear_3 = nn.Linear(in_features = self.nodes * 4, out_features = self.nodes)\n",
    "        self.output = nn.Linear(in_features = self.nodes, out_features = 1)\n",
    "\n",
    "    def forward(self, tokens, experiment_indicator, self_attn_padding_mask=None):\n",
    "        # ESM embedding\n",
    "        embeddings = self.esm2(tokens, repr_layers, return_representation=True)\n",
    "        embeddings_rep = embeddings[\"representations\"][layers][:, 1 : -1] #B*(T+2)*E -> B*T*E\n",
    "\n",
    "        for i, layer in enumerate(self.convtransformer_decoder):\n",
    "            x_o, attn = layer(x=embeddings_rep, self_attn_padding_mask=self_attn_padding_mask)  #tokens: B*T*E, x_o: B*T*E\n",
    "\n",
    "        x = torch.flip(x_o, dims=[1])  # Reverse along the sequence length dimension\n",
    "        # Select frames corresponding to frame 1, frame 2, and frame 3\n",
    "        frame_1 = x[:, 0::3, :]\n",
    "        frame_2 = x[:, 1::3, :]\n",
    "        frame_3 = x[:, 2::3, :]\n",
    "        # Global Max Pooling\n",
    "        frame_1_max = torch.max(frame_1, dim=1)[0]  # B*C\n",
    "        frame_2_max = torch.max(frame_2, dim=1)[0]  # B*C\n",
    "        frame_3_max = torch.max(frame_3, dim=1)[0]  # B*C\n",
    "        # Expand the dimensions of self_attn_padding_mask to match the feature tensor\n",
    "        mask_expanded = ~self_attn_padding_mask.unsqueeze(2)  # (batch_size, seq_len, 1)ï¼ŒTrue Indicates valid data\n",
    "        # Mean pooling over valid positions\n",
    "        def masked_mean(frame, mask):\n",
    "            frame_sum = torch.sum(frame * mask, dim=1)\n",
    "            mask_sum = torch.sum(mask, dim=1) + 1e-8  # Avoid dividing by zero\n",
    "            return frame_sum / mask_sum\n",
    "        # Global average pooling\n",
    "        frame_1_avg = masked_mean(frame_1, mask_expanded[:, 0::3, :])\n",
    "        frame_2_avg = masked_mean(frame_2, mask_expanded[:, 1::3, :])\n",
    "        frame_3_avg = masked_mean(frame_3, mask_expanded[:, 2::3, :])\n",
    "        # Concatenate the pooled tensors into a single tensor\n",
    "        pooled_output = torch.cat([frame_1_max, frame_1_avg, frame_2_max, frame_2_avg, frame_3_max, frame_3_avg], dim=1)  # B*(6*C)\n",
    "        # Linear layer processing experiment indicator\n",
    "        experiment_output = self.experiment_dense(experiment_indicator)\n",
    "        x_pooled = self.flatten(pooled_output)\n",
    "\n",
    "        o_linear = self.linear(x_pooled) + experiment_output #Concatenate the pooling output with experimental information\n",
    "        o_linear_2 = self.linear_2(o_linear)\n",
    "        o_linear_3 = self.linear_3(o_linear_2)\n",
    "\n",
    "        o_relu = self.relu(o_linear_3)\n",
    "        o_dropout = self.dropout(o_relu)\n",
    "        o = self.output(o_dropout)  # B*1\n",
    "\n",
    "        return o\n",
    "    \n",
    "    \n",
    "# test the MainModel\n",
    "tokens = torch.tensor([[7, 4, 6, 3, 6, 6, 5, 3, 5, 4, 5, 5, 4, 5, 6, 5, 3, 5, 3, 3, 3, 6, 3, 6,\n",
    "         4, 4, 3, 6, 6, 6, 5, 5, 4, 6, 5, 6, 4, 6, 6, 5, 3, 5, 3, 6, 6, 6, 6, 6,\n",
    "         3, 5, 3, 4, 4, 4, 5, 5, 3, 5, 5, 6, 3, 5, 5, 5, 5, 3, 6, 4, 6, 3, 4, 3,\n",
    "         3, 6, 4, 5, 3, 5, 3, 4, 3, 3, 6, 6, 3, 5, 5, 6, 5, 3, 6, 5, 3, 6, 3, 3,\n",
    "         6, 4, 6, 5, 3, 1],\n",
    "        [7, 4, 3, 6, 5, 3, 3, 3, 5, 6, 6, 3, 5, 5, 4, 6, 3, 6, 6, 3, 4, 5, 5, 5,\n",
    "         4, 3, 5, 5, 5, 6, 4, 3, 5, 6, 6, 4, 6, 5, 3, 5, 4, 3, 5, 6, 4, 5, 4, 5,\n",
    "         4, 5, 5, 3, 4, 3, 5, 6, 5, 5, 5, 5, 5, 5, 5, 4, 4, 6, 4, 3, 4, 3, 6, 6,\n",
    "         4, 6, 6, 6, 5, 5, 5, 4, 4, 5, 4, 5, 4, 6, 6, 3, 5, 3, 4, 4, 5, 3, 6, 5,\n",
    "         5, 3, 3, 3, 6, 1],\n",
    "        [7, 3, 3, 5, 6, 6, 3, 5, 3, 3, 3, 4, 3, 6, 6, 4, 4, 6, 3, 4, 3, 3, 5, 6,\n",
    "         3, 4, 3, 3, 3, 3, 3, 3, 6, 5, 3, 6, 6, 6, 3, 3, 6, 3, 5, 6, 3, 3, 5, 4,\n",
    "         4, 3, 3, 3, 3, 3, 5, 6, 5, 6, 5, 3, 6, 5, 5, 5, 6, 3, 3, 4, 5, 5, 5, 3,\n",
    "         3, 6, 4, 6, 5, 6, 3, 4, 4, 3, 3, 5, 5, 6, 3, 6, 3, 3, 3, 4, 3, 3, 3, 5,\n",
    "         6, 6, 4, 3, 6, 1],\n",
    "        [7, 6, 3, 4, 6, 6, 4, 4, 4, 4, 3, 4, 3, 6, 3, 6, 3, 5, 6, 5, 5, 6, 6, 5,\n",
    "         4, 4, 4, 5, 5, 3, 6, 4, 3, 4, 5, 6, 4, 4, 3, 5, 5, 4, 3, 3, 4, 5, 3, 6,\n",
    "         4, 5, 3, 6, 3, 5, 4, 6, 6, 6, 6, 6, 4, 6, 3, 4, 3, 6, 5, 4, 6, 6, 5, 6,\n",
    "         6, 3, 6, 4, 3, 5, 4, 4, 4, 5, 6, 5, 4, 3, 4, 5, 4, 6, 3, 6, 3, 5, 3, 5,\n",
    "         4, 6, 5, 6, 6, 1],\n",
    "        [7, 4, 3, 3, 4, 3, 6, 5, 6, 6, 4, 5, 6, 4, 5, 5, 3, 6, 6, 4, 4, 5, 6, 5,\n",
    "         6, 6, 5, 3, 3, 3, 5, 3, 3, 5, 6, 3, 5, 3, 4, 3, 3, 4, 6, 6, 3, 4, 4, 4,\n",
    "         4, 4, 5, 6, 3, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 3, 3, 6, 4, 3, 3,\n",
    "         5, 5, 5, 6, 5, 3, 6, 6, 4, 6, 6, 5, 4, 5, 3, 5, 5, 6, 6, 6, 5, 4, 4, 3,\n",
    "         3, 3, 6, 5, 3, 1]]).to(device)\n",
    "\n",
    "experiment_indicator = torch.tensor([[0., 1.],\n",
    "        [0., 1.],\n",
    "        [0., 1.],\n",
    "        [0., 1.],\n",
    "        [0., 1.]]).to(device)\n",
    "\n",
    "self_attn_padding_mask = torch.tensor([[False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False],\n",
    "        [False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False],\n",
    "        [False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False],\n",
    "        [False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False],\n",
    "        [False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False]]).to(device)\n",
    "\n",
    "alphabet = Alphabet(mask_prob = 0.0, standard_toks = 'AGCT')\n",
    "\n",
    "model = MainModel(alphabet).to(device)\n",
    "model(tokens, experiment_indicator, self_attn_padding_mask=self_attn_padding_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46b1bd",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf55e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(x,y):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    return r_value**2\n",
    "\n",
    "def performances(label, pred):\n",
    "    label, pred = list(label), list(pred)\n",
    "    r = r2(label, pred)\n",
    "    R2 = r2_score(label, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(label, pred))\n",
    "    mae = mean_absolute_error(label, pred)\n",
    "    try:\n",
    "        pearson_r = pearsonr(label, pred)[0]\n",
    "    except:\n",
    "        pearson_r = -1e-9\n",
    "    try:\n",
    "        sp_cor = spearmanr(label, pred)[0]\n",
    "    except:\n",
    "        sp_cor = -1e-9\n",
    "    print(f'r-squared = {r:.4f} | pearson r = {pearson_r:.4f} | spearman R = {sp_cor:.4f} | R-squared = {R2:.4f} | RMSE = {rmse:.4f} | MAE = {mae:.4f}')\n",
    "    return [r, pearson_r, sp_cor, R2, rmse, mae]\n",
    "\n",
    "def performances_to_pd(performances_list):\n",
    "    performances_pd = pd.DataFrame(performances_list, index = ['r2', 'PearsonR', 'SpearmanR', 'R2', 'RMSE', 'MAE']).T\n",
    "    return performances_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d19656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_dataloader(e_data, obj_col, lab_col, batch_toks=8192*4, mask_prob = 0.0):\n",
    "    dataset = FastaBatchedDataset(e_data.loc[:,obj_col], e_data.loc[:, lab_col], mask_prob = mask_prob)\n",
    "    batches = dataset.get_batch_indices(toks_per_batch=batch_toks, extra_toks_per_seq=2)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                            collate_fn=alphabet.get_batch_converter(), \n",
    "                                            batch_sampler=batches, \n",
    "                                            shuffle = False)\n",
    "    print(f\"{len(dataset)} sequences\")\n",
    "    return dataset, dataloader, batches\n",
    "\n",
    "def get_experiment_indicator_for_batch(data_combine, batch_idx):\n",
    "    # Get the experiment_indicator for the corresponding batch from train_combine\n",
    "    batch_experiment_indicators = data_combine.iloc[batch_idx]['experiment_indicator'].values.tolist()\n",
    "    # Convert to tensor\n",
    "    experiment_indicator_tensor = torch.tensor(batch_experiment_indicators, dtype=torch.float32).to(device)\n",
    "    return experiment_indicator_tensor\n",
    "\n",
    "def shuffle_data_fn(in_data):\n",
    "    # Use sample(frac=1) to shuffle the order of the dataset\n",
    "    shuffle_data = in_data.sample(frac=1).reset_index(drop=True)\n",
    "    return shuffle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5babebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_dataloader, train_shuffle_combine, train_shuffle_batch, model, epoch):        \n",
    "    model.train()\n",
    "    y_pred_list, y_true_list, loss_list = [], [], []\n",
    "    \n",
    "    for i, (labels, strs, masked_strs, toks, masked_toks, _) in enumerate(tqdm(train_dataloader)):\n",
    "        toks = toks.to(device)\n",
    "        padding_mask = toks.eq(alphabet.padding_idx)[:, 1:-1]\n",
    "        labels = torch.FloatTensor(labels).to(device).reshape(-1, 1)\n",
    "        experiment_indicator_tensor = get_experiment_indicator_for_batch(train_shuffle_combine, train_shuffle_batch[i])\n",
    "\n",
    "        outputs= model(toks, experiment_indicator_tensor, self_attn_padding_mask=padding_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.cpu().detach())\n",
    "\n",
    "        y_true_list.extend(labels.cpu().reshape(-1).tolist())\n",
    "        y_pred = outputs.reshape(-1).cpu().detach().tolist()\n",
    "        y_pred_list.extend(y_pred)\n",
    "        \n",
    "    loss_epoch = float(torch.Tensor(loss_list).mean())\n",
    "    print(f'Train: Epoch-{epoch}/{num_epochs} | Loss = {loss_epoch:.4f} | ', end = '')\n",
    "    \n",
    "    metrics = performances(y_true_list, y_pred_list)\n",
    "    return metrics, loss_epoch\n",
    "\n",
    "\n",
    "def eval_step(test_dataloader, test_combine, test_batch, model, epoch):\n",
    "    model.eval()\n",
    "    y_pred_list, y_true_list, loss_list = [], [], []\n",
    "    strs_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (labels, strs, masked_strs, toks, masked_toks, _) in enumerate(tqdm(test_dataloader)):\n",
    "            strs_list.extend(strs)\n",
    "            toks = toks.to(device)\n",
    "            padding_mask = toks.eq(alphabet.padding_idx)[:, 1:-1]\n",
    "            labels = torch.FloatTensor(labels).to(device).reshape(-1, 1)\n",
    "            experiment_indicator_tensor = get_experiment_indicator_for_batch(test_combine, test_batch[i])\n",
    "            \n",
    "            outputs= model(toks, experiment_indicator_tensor, self_attn_padding_mask=padding_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(loss.cpu().detach())\n",
    "\n",
    "            y_pred = outputs.reshape(-1).cpu().detach().tolist()\n",
    "            y_true_list.extend(labels.cpu().reshape(-1).tolist())\n",
    "            y_pred_list.extend(y_pred)\n",
    "        \n",
    "        loss_epoch = float(torch.Tensor(loss_list).mean())\n",
    "        print(f'Test: Epoch-{epoch}/{num_epochs} | Loss = {loss_epoch:.4f} | ', end = '')\n",
    "        metrics = performances(y_true_list, y_pred_list)\n",
    "        e_pred = pd.DataFrame([strs_list, y_true_list, y_pred_list], index = ['utr', 'y_true', 'y_pred']).T\n",
    "        \n",
    "    return metrics, loss_epoch, e_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24398410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Load Data====\n"
     ]
    }
   ],
   "source": [
    "print('====Load Data====')\n",
    "train_data_50_random = pd.read_csv(f'Data/train_test/4.1_train_data_GSM3130435_egfp_unmod_1_BiologyFeatures.csv')\n",
    "train_data_vary_random = pd.read_csv(f'Data/train_test/VaryLengthRandomTrain_sequence.csv')\n",
    "test_data_50_random = pd.read_csv(f'Data/train_test/4.1_test_data_GSM3130435_egfp_unmod_1.csv')\n",
    "test_data_vary_random = pd.read_csv(f'Data/train_test/VaryLengthRandomTest_sequence_num7600.csv')\n",
    "eval_data_vary_human = pd.read_csv(f'Data/train_test/VaryLengthHumanAll_sequence_num15555.csv')\n",
    "\n",
    "# train data\n",
    "# Add 50 <pad> to the left side of the utr column\n",
    "train_data_50_random['utr_100'] = '<pad>'*50 + train_data_50_random['utr']\n",
    "test_data_50_random['utr_100'] = '<pad>'*50 + test_data_50_random['utr']\n",
    "\n",
    "# Select the 'rl' and 'utr' columns\n",
    "train_data_50_selected = train_data_50_random[['rl', 'utr_100']]\n",
    "train_data_vary_selected = train_data_vary_random[['rl', 'utr_100']]\n",
    "\n",
    "# Add experimental indicators for train_data_50_selected and train_data_vary_selected\n",
    "train_data_50_selected.loc[:, 'experiment_indicator'] = [[1, 0]] * len(train_data_50_selected)\n",
    "train_data_vary_selected.loc[:, 'experiment_indicator'] = [[0, 1]] * len(train_data_vary_selected)\n",
    "\n",
    "# Merge two datasets\n",
    "train_combine = pd.concat([train_data_50_selected, train_data_vary_selected], ignore_index=True)\n",
    "\n",
    "# test data\n",
    "# Select the 'rl' and 'utr' columns\n",
    "test_data_50_selected = test_data_50_random[['rl', 'utr_100']]\n",
    "test_data_vary_selected = test_data_vary_random[['rl', 'utr_100']]\n",
    "# Add experimental indicators for test_data_50_selected, test_data_vary_selected, and eval_human_selected\n",
    "test_data_50_selected.loc[:, 'experiment_indicator'] = [[1, 0]] * len(test_data_50_selected)\n",
    "test_data_vary_selected.loc[:, 'experiment_indicator'] = [[0, 1]] * len(test_data_vary_selected)\n",
    "# Merge two datasets\n",
    "test_combine = pd.concat([test_data_50_selected, test_data_vary_selected], ignore_index=True)\n",
    "\n",
    "# eval data\n",
    "eval_human_selected = eval_data_vary_human[['rl', 'utr_100']]\n",
    "eval_human_selected.loc[:, 'experiment_indicator'] = [[0, 1]] * len(eval_human_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ada21630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260000\n",
      "76319\n",
      "20000\n",
      "7600\n",
      "15555\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_50_random))\n",
    "print(len(train_data_vary_random))\n",
    "print(len(test_data_50_random))\n",
    "print(len(test_data_vary_random))\n",
    "print(len(eval_data_vary_human))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b10523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<eos>': 1, '<unk>': 2, 'A': 3, 'G': 4, 'C': 5, 'T': 6, '<cls>': 7, '<mask>': 8, '<sep>': 9}\n"
     ]
    }
   ],
   "source": [
    "alphabet = Alphabet(mask_prob = 0.0, standard_toks = 'AGCT')\n",
    "print(alphabet.tok_to_idx)\n",
    "assert alphabet.tok_to_idx == {'<pad>': 0, '<eos>': 1, '<unk>': 2, 'A': 3, 'G': 4, 'C': 5, 'T': 6, '<cls>': 7, '<mask>': 8, '<sep>': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c534fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>utr</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>codon_P</th>\n",
       "      <th>codon_R</th>\n",
       "      <th>codon_V</th>\n",
       "      <th>codon_W</th>\n",
       "      <th>codon_N</th>\n",
       "      <th>uORF</th>\n",
       "      <th>CGperc</th>\n",
       "      <th>utrlen_m80</th>\n",
       "      <th>ATratio</th>\n",
       "      <th>utr_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20000</td>\n",
       "      <td>CCGGCTATAGCGCGCAGTGCTCGGATGGCAAGGCGTTCAACCGTGA...</td>\n",
       "      <td>9.131133e-06</td>\n",
       "      <td>9.043781e-06</td>\n",
       "      <td>1.146974e-05</td>\n",
       "      <td>1.399193e-05</td>\n",
       "      <td>1.112620e-05</td>\n",
       "      <td>7.725591e-06</td>\n",
       "      <td>4.959113e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20001</td>\n",
       "      <td>AATGGGTACCGTATGTCAAAGGAACCATAGGCGCGGTCGTAGTGGA...</td>\n",
       "      <td>1.675790e-05</td>\n",
       "      <td>1.469614e-05</td>\n",
       "      <td>1.625375e-05</td>\n",
       "      <td>9.543211e-06</td>\n",
       "      <td>6.772468e-06</td>\n",
       "      <td>4.363158e-06</td>\n",
       "      <td>3.449818e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20002</td>\n",
       "      <td>CCGCGGCATGATCAAACGGTCTAGATCATAATGCGAGGTCAGTACC...</td>\n",
       "      <td>1.109030e-05</td>\n",
       "      <td>1.347871e-05</td>\n",
       "      <td>1.423318e-05</td>\n",
       "      <td>1.198283e-05</td>\n",
       "      <td>8.126962e-06</td>\n",
       "      <td>6.484693e-06</td>\n",
       "      <td>4.168530e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20003</td>\n",
       "      <td>GTGTAAGGCTCTGGTGGGCACACGTCGGTTTGCAGTAATGACAGAC...</td>\n",
       "      <td>1.448387e-05</td>\n",
       "      <td>1.843540e-05</td>\n",
       "      <td>1.509489e-05</td>\n",
       "      <td>9.220320e-06</td>\n",
       "      <td>9.723329e-06</td>\n",
       "      <td>6.004345e-06</td>\n",
       "      <td>2.515492e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20004</td>\n",
       "      <td>CCGGAAAATCGCGCAAGTCTCCCTTACGAATAAAGCTCAAAGGGAA...</td>\n",
       "      <td>1.399407e-06</td>\n",
       "      <td>2.087026e-06</td>\n",
       "      <td>1.753147e-06</td>\n",
       "      <td>4.628098e-06</td>\n",
       "      <td>5.272850e-06</td>\n",
       "      <td>6.965040e-06</td>\n",
       "      <td>8.121447e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259995</th>\n",
       "      <td>259995</td>\n",
       "      <td>279995</td>\n",
       "      <td>TCGCTTTGAGGCGCACGTGAAACGCAGCAACCCCCTCGAAGTTGTC...</td>\n",
       "      <td>3.148667e-07</td>\n",
       "      <td>5.217566e-07</td>\n",
       "      <td>3.862867e-07</td>\n",
       "      <td>2.511371e-07</td>\n",
       "      <td>4.837477e-07</td>\n",
       "      <td>1.040753e-06</td>\n",
       "      <td>7.546477e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259996</th>\n",
       "      <td>259996</td>\n",
       "      <td>279996</td>\n",
       "      <td>CGCTAGAACCAGGAGCAACGTAGCGTAAGGGGAATATGAAGGAAAA...</td>\n",
       "      <td>2.448963e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.152604e-07</td>\n",
       "      <td>1.064245e-06</td>\n",
       "      <td>9.606952e-07</td>\n",
       "      <td>1.006197e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259997</th>\n",
       "      <td>259997</td>\n",
       "      <td>279997</td>\n",
       "      <td>GGTGAACGACGAGACAATAACGTGGGCCTTTAAAGGAATAGGGCAA...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.228906e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.120811e-06</td>\n",
       "      <td>6.827765e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259998</th>\n",
       "      <td>259998</td>\n",
       "      <td>279998</td>\n",
       "      <td>CCGGAGTATTACATCACACATGAGCCCGGTGGGGATGTTGGGGAGC...</td>\n",
       "      <td>1.784244e-06</td>\n",
       "      <td>2.347905e-06</td>\n",
       "      <td>1.782862e-06</td>\n",
       "      <td>1.435069e-06</td>\n",
       "      <td>2.418739e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.030985e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259999</th>\n",
       "      <td>259999</td>\n",
       "      <td>279999</td>\n",
       "      <td>CCGATCTATCGCGATCAAAGAACGAGAAAACCTCGACTAGGTGATA...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.565537e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.865590e-06</td>\n",
       "      <td>2.418739e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.371750e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260000 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.1  Unnamed: 0  \\\n",
       "0                  0       20000   \n",
       "1                  1       20001   \n",
       "2                  2       20002   \n",
       "3                  3       20003   \n",
       "4                  4       20004   \n",
       "...              ...         ...   \n",
       "259995        259995      279995   \n",
       "259996        259996      279996   \n",
       "259997        259997      279997   \n",
       "259998        259998      279998   \n",
       "259999        259999      279999   \n",
       "\n",
       "                                                      utr             0  \\\n",
       "0       CCGGCTATAGCGCGCAGTGCTCGGATGGCAAGGCGTTCAACCGTGA...  9.131133e-06   \n",
       "1       AATGGGTACCGTATGTCAAAGGAACCATAGGCGCGGTCGTAGTGGA...  1.675790e-05   \n",
       "2       CCGCGGCATGATCAAACGGTCTAGATCATAATGCGAGGTCAGTACC...  1.109030e-05   \n",
       "3       GTGTAAGGCTCTGGTGGGCACACGTCGGTTTGCAGTAATGACAGAC...  1.448387e-05   \n",
       "4       CCGGAAAATCGCGCAAGTCTCCCTTACGAATAAAGCTCAAAGGGAA...  1.399407e-06   \n",
       "...                                                   ...           ...   \n",
       "259995  TCGCTTTGAGGCGCACGTGAAACGCAGCAACCCCCTCGAAGTTGTC...  3.148667e-07   \n",
       "259996  CGCTAGAACCAGGAGCAACGTAGCGTAAGGGGAATATGAAGGAAAA...  2.448963e-07   \n",
       "259997  GGTGAACGACGAGACAATAACGTGGGCCTTTAAAGGAATAGGGCAA...  0.000000e+00   \n",
       "259998  CCGGAGTATTACATCACACATGAGCCCGGTGGGGATGTTGGGGAGC...  1.784244e-06   \n",
       "259999  CCGATCTATCGCGATCAAAGAACGAGAAAACCTCGACTAGGTGATA...  0.000000e+00   \n",
       "\n",
       "                   1             2             3             4             5  \\\n",
       "0       9.043781e-06  1.146974e-05  1.399193e-05  1.112620e-05  7.725591e-06   \n",
       "1       1.469614e-05  1.625375e-05  9.543211e-06  6.772468e-06  4.363158e-06   \n",
       "2       1.347871e-05  1.423318e-05  1.198283e-05  8.126962e-06  6.484693e-06   \n",
       "3       1.843540e-05  1.509489e-05  9.220320e-06  9.723329e-06  6.004345e-06   \n",
       "4       2.087026e-06  1.753147e-06  4.628098e-06  5.272850e-06  6.965040e-06   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "259995  5.217566e-07  3.862867e-07  2.511371e-07  4.837477e-07  1.040753e-06   \n",
       "259996  0.000000e+00  0.000000e+00  2.152604e-07  1.064245e-06  9.606952e-07   \n",
       "259997  0.000000e+00  0.000000e+00  3.228906e-07  0.000000e+00  1.120811e-06   \n",
       "259998  2.347905e-06  1.782862e-06  1.435069e-06  2.418739e-07  0.000000e+00   \n",
       "259999  9.565537e-07  0.000000e+00  1.865590e-06  2.418739e-06  0.000000e+00   \n",
       "\n",
       "                   6  ...  codon_P  codon_R  codon_V  codon_W  codon_N  uORF  \\\n",
       "0       4.959113e-06  ...   0.0625   0.2500   0.0625   0.0000   0.0000   0.0   \n",
       "1       3.449818e-06  ...   0.0625   0.1250   0.0000   0.0625   0.0625   2.0   \n",
       "2       4.168530e-06  ...   0.0625   0.1250   0.0625   0.0000   0.0000   1.0   \n",
       "3       2.515492e-06  ...   0.0000   0.0625   0.1250   0.0000   0.0625   1.0   \n",
       "4       8.121447e-06  ...   0.1250   0.0625   0.0000   0.0000   0.0625   1.0   \n",
       "...              ...  ...      ...      ...      ...      ...      ...   ...   \n",
       "259995  7.546477e-07  ...   0.1250   0.1250   0.0000   0.0000   0.0000   1.0   \n",
       "259996  1.006197e-06  ...   0.0000   0.2500   0.0625   0.0000   0.0625   2.0   \n",
       "259997  6.827765e-07  ...   0.0000   0.2500   0.0625   0.0000   0.1875   1.0   \n",
       "259998  5.030985e-07  ...   0.1250   0.0000   0.0625   0.0000   0.0000   0.0   \n",
       "259999  2.371750e-06  ...   0.1250   0.2500   0.0000   0.0000   0.0000   0.0   \n",
       "\n",
       "        CGperc  utrlen_m80   ATratio  \\\n",
       "0         0.68        30.0  0.222222   \n",
       "1         0.56        30.0  0.363636   \n",
       "2         0.54        30.0  0.454545   \n",
       "3         0.58        30.0  0.076923   \n",
       "4         0.54        30.0  1.000000   \n",
       "...        ...         ...       ...   \n",
       "259995    0.58        30.0  0.272727   \n",
       "259996    0.50        30.0  2.142857   \n",
       "259997    0.48        30.0  1.333333   \n",
       "259998    0.62        30.0  0.300000   \n",
       "259999    0.48        30.0  1.000000   \n",
       "\n",
       "                                                  utr_100  \n",
       "0       <pad><pad><pad><pad><pad><pad><pad><pad><pad><...  \n",
       "1       <pad><pad><pad><pad><pad><pad><pad><pad><pad><...  \n",
       "2       <pad><pad><pad><pad><pad><pad><pad><pad><pad><...  \n",
       "3       <pad><pad><pad><pad><pad><pad><pad><pad><pad><...  \n",
       "4       <pad><pad><pad><pad><pad><pad><pad><pad><pad><...  \n",
       "...                                                   ...  \n",
       "259995  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...  \n",
       "259996  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...  \n",
       "259997  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...  \n",
       "259998  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...  \n",
       "259999  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...  \n",
       "\n",
       "[260000 rows x 62 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_50_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9408f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336319 sequences\n",
      "27600 sequences\n",
      "15555 sequences\n"
     ]
    }
   ],
   "source": [
    "train_shuffle_data = shuffle_data_fn(train_combine)\n",
    "train_shuffle_dataset,  train_shuffle_dataloader, train_shuffle_batch = generate_dataset_dataloader(train_shuffle_data, 'rl', 'utr_100', mask_prob = 0.0)\n",
    "test_dataset, test_dataloader, test_batch = generate_dataset_dataloader(test_combine, 'rl', 'utr_100', mask_prob = 0.0)\n",
    "eval_dataset, eval_dataloader, eval_batch = generate_dataset_dataloader(eval_human_selected, 'rl', 'utr_100', mask_prob = 0.0)\n",
    "\n",
    "esm2_modelfile = 'Model/utr_lm/ESM2SISS_FS4.22_fiveSpeciesCao_6layers_16heads_128embedsize_4096batchToks_lr1e-05_supervisedweight1.0_structureweight1.0_MLMLossMin_epoch115.pkl'\n",
    "\n",
    "model = MainModel(alphabet).to(device)\n",
    "state_dict = torch.load(esm2_modelfile, map_location=device)\n",
    "model.esm2.load_state_dict({k.replace('module.', ''):v for k,v in state_dict.items()})\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "learning_rate = 1e-4 #1e-4, 1e-05\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate, \n",
    "    momentum=0.9,\n",
    "    weight_decay = 1e-4)\n",
    "\n",
    "criterion = torch.nn.HuberLoss()\n",
    "\n",
    "loss_best, ep_best, r2_best = np.inf, -1, -1\n",
    "loss_train_dict, loss_test_dict = dict(), dict()\n",
    "loss_valid_dict = dict() \n",
    "\n",
    "metrics_train_dict = dict()\n",
    "metrics_test_dict = dict()\n",
    "metrics_valid_dict = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designated training information\n",
    "train_info = 'train_info'\n",
    "folder_path = f\"{train_info}/\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder created: {folder_path}\")\n",
    "else:\n",
    "    print(f\"The folder already exists: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f83058c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108298240\n",
      "117440512\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    metrics_train, loss_train = train_step(train_shuffle_dataloader, train_shuffle_data, train_shuffle_batch, model, epoch)\n",
    "    loss_train_dict[epoch] = loss_train\n",
    "    metrics_train_dict[epoch] = metrics_train\n",
    "    \n",
    "    metrics_test, loss_test, _ = eval_step(test_dataloader, test_combine, test_batch, model, epoch)\n",
    "    loss_test_dict[epoch] = loss_test\n",
    "    metrics_test_dict[epoch] = metrics_test\n",
    "\n",
    "    if metrics_test[0] > r2_best: \n",
    "        path_saver = f'{train_info}/model_epoch{epoch}.pkl'\n",
    "        r2_best, ep_best = metrics_test[0], epoch\n",
    "        torch.save(model.eval().state_dict(), path_saver)\n",
    "        print(f'****Saving model in {path_saver}: Best epoch = {ep_best} | Train Loss = {loss_train:.4f} |  Val Loss = {loss_test:.4f} | R2_best = {r2_best:.4f}')\n",
    "        \n",
    "    metrics_human, loss_human, _ = eval_step(eval_dataloader, eval_human_selected, eval_batch, model, epoch)\n",
    "    loss_valid_dict[epoch] = loss_test\n",
    "    metrics_valid_dict[epoch] = metrics_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efbc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_df = pd.DataFrame(loss_train_dict,\n",
    "                             index = ['train_loss']).T\n",
    "loss_test_df = pd.DataFrame(loss_test_dict,\n",
    "                            index = ['test_loss']).T\n",
    "loss_valid_df = pd.DataFrame(loss_valid_dict,\n",
    "                            index = ['valid_loss']).T\n",
    "\n",
    "\n",
    "metrics_train_df = pd.DataFrame(metrics_train_dict, \n",
    "                                index = [\n",
    "                                        'Train_r2', 'Train_PearsonR', 'Train_SpearmanR', 'Train_R2', 'Train_RMSE', 'Train_MAE'\n",
    "                                        ]).T\n",
    "\n",
    "metrics_test_df = pd.DataFrame(metrics_test_dict, \n",
    "                                index = [\n",
    "                                        'test_r2', 'test_PearsonR', 'test_SpearmanR', 'test_R2', 'test_RMSE', 'test_MAE'\n",
    "                                        ]).T\n",
    "metrics_valid_df = pd.DataFrame(metrics_valid_dict, \n",
    "                                index = [\n",
    "                                        'valid_r2', 'valid_PearsonR', 'valid_SpearmanR', 'valid_R2', 'valid_RMSE', 'valid_MAE'\n",
    "                                        ]).T\n",
    "# Save training and testing loss\n",
    "loss_train_df.to_csv(f'{train_info}/train_loss.csv')\n",
    "loss_test_df.to_csv(f'{train_info}/test_loss.csv')\n",
    "loss_valid_df.to_csv(f'{train_info}/human_loss.csv')\n",
    "\n",
    "metrics_train_df.to_csv(f'{train_info}/train_metrics.csv', index = True)\n",
    "metrics_test_df.to_csv(f'{train_info}/test_metrics.csv', index = True)\n",
    "metrics_valid_df.to_csv(f'{train_info}/human_metrics.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
